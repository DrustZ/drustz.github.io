---
layout: page
---
<head>
<!--    for publication     -->
    <style type="text/css">
        @media screen and (prefers-reduced-motion: reduce) {
            html {
                scroll-behavior: auto;
            }
        }

        html {
            scroll-behavior: smooth;
        }
        table.row td{
            background-color: transparent !important;
            border: 0;
            vertical-align: top;
        }
        li{
            list-style-type: none;
        }
        a.paper_link
        {
            display: block; 
            text-decoration: none;
            font-weight: bold;
        }
        
        @media only screen and (max-device-width: 767px) { 
            .imgtd {
                display: none;
            }
            img.smallthumbnail {
                display: block;
                margin-top: 3em;
            }
        }
        
        
        @media only screen and (min-device-width: 768px) { 
            img.smallthumbnail {
                display: none;
            }
        }
        
        .imgtd {
            width: 35%;
        }
        
        img.thumbnail
        {
            margin-top: 15px;
            padding-left: 10px;
            border-radius: 10px;
        }
        
        img.inline {
            display: inline;
            margin: 0;
        }
        
       .title {
            font-size: large;
            font-family: sans-serif;
        }
        .description{
            font-size: medium;
        }
        .detail{
            font-size: small;
        }
    </style>
</head>

<!-- MATERIALS -->
Some of my research materials: 
<ul>
    <li>My Ph.D. Annual Review Presentation Slides <a href="https://drive.google.com/file/d/1Tl13_9e-T3lr6SB4I1u8AsBhRBXxSqZm/view?usp=sharing">1st year</a> <a href="https://drive.google.com/file/d/1pe9Yk4j-Kjbx-vglPhuNDkiGA5EocMvH/view?usp=sharing" target="_blank">2nd year</a></li>
    <li>My <a href="/assets/pdfs/Mingrui_Thesis_Proposal.pdf" target="_blank">Thesis Proposal</a> Presentation <a href="https://youtu.be/QX05YleSHrA" target="_blank">on Youtube</a> /<a href="https://www.bilibili.com/video/BV15V411e7zZ/" target="_blank">on Bilibili</a> /<a href="https://drive.google.com/file/d/1gt_RXemckMtKKUWSmJ1yCM95MPBIUoWn/view?usp=sharing" target="_blank">Slides</a></li>
    <li>My <a href="assets/pdfs/Research_Statement_Mingrui_Zhang.pdf" target="_blank">Research Statement</a> and <a href="assets/pdfs/Teaching_Statement_Mingrui_Zhang.pdf" target="_blank">Teaching Statement</a></li>
</ul>

<h2>Categories</h2>
<ul>
    <li><a href="#mobile">Intelligent Mobile Text Entry Interactions üì±</a></li>
    <li><a href="#ubiquitous">Ubiquitous Text Entry üôå</a></li>
    <li><a href="#eval">Evalutating Text-based Communication Systems üßê</a></li>
    <li><a href="#a11y">Assistive Digital Communication Systems üë®‚Äçü¶Ø</a></li>
    <li><a href="#assistant">Talking with Smart Assistants ü§ñ</a></li>
    <li><a href="#wellbeing">Digital Information Wellbeing üßòüèª</a></li>
    <li><a href="#misc">MISC üí°</a></li>
</ul>

<!-- PUBLICATIONS -->
<!-- <h2><strong>Publications</strong></h2> -->
<h2 id="mobile"> Intelligent Mobile Text Entry Interactions üì±</h2>
<div class="hr" style="padding-bottom:0"></div>
    <table class="row">
        <!-- start of proj -->
        <img class="smallthumbnail" src="/assets/img/research/phraseflow.png" alt="" >
        <tr><td class="imgtd">
            <img class="thumbnail" src="/assets/img/research/phraseflow.png" alt="" >
        </td><td>
        <div class="descol">
            <li><span class="title"><a class="paper_link" href="/assets/pdfs/phraseflow.pdf" target="_blank"> PhraseFlow: Designs and Empirical Studies of Phrase-Level Input </a></span>
                <span class="description"><strong>Mingrui Zhang</strong>, Shumin Zhai
                    <br>
                    The ACM CHI Conference on Human Factors in Computing Systems <strong>(CHI)</strong>, 2021 <a href="https://www.youtube.com/watch?v=6e0_TcTNIdA" target="_blank">Video</a> <a href="https://youtu.be/rcWJixyrXX0" target="_blank">My presentation</a>
                    <br>
                    <font color="green" class="detail">PhraseFlow is a phrase-level input keyboard that is able to correct previous text based on the subsequently input sequences. For example, if the user types "I love in Seattle", the keyboard will correct "love" to "live" after it sees "in Seattle". The project evaluated the usability of PhraseFlow and attempted to design a pracitcally usable phrase-level keyboard. </font>
            </span></li>
        </div></td></tr>
        <!-- end of proj -->

        <!-- start of proj -->
        <img class="smallthumbnail" src="/assets/img/research/JustCorrect.png" alt="" >
        <tr><td class="imgtd">
            <img class="thumbnail" src="/assets/img/research/JustCorrect.png" alt="" >
        </td><td>
        <div class="descol">
            <li><span class="title"><a class="paper_link" href="/assets/pdfs/justcorrect.pdf" target="_blank"> JustCorrect: Intelligent Post Hoc Text Correction Techniques on Smartphones </a></span>
                <span class="description">Wenzhe Cui, Suwen Zhu, <strong>Mingrui Zhang</strong>, H. Andrew Schwartz, Jacob O. Wobbrock, Xiaojun Bi
                    <br>
                    The ACM Symposium on User Interface Software and Technology <strong>(UIST)</strong>, 2020 <a href="https://www.youtube.com/watch?v=x1OH-uTT524" target="_blank">Demo</a> <a href="https://www.youtube.com/watch?v=gdCryupTzpg" target="_blank">Wenzhe's presentation</a>
                    <br>
                    <font color="green" class="detail">JustCorrect is a post-correction technique for smartphones, sharing the same genre with <i>Type, Then Correct</i>. JustCorrect utilizes the word embeddings and language models to detect the error and display correction options automatically.</font>
            </span></li>
        </div></td></tr>
        <!-- end of proj -->

        <img class="smallthumbnail" src="/assets/img/research/gedit.png" alt="" >
        <tr><td class="imgtd">
            <img class="thumbnail" src="/assets/img/research/gedit.png" alt="" >
        </td><td>
        <div class="descol">
            <li><span class="title"><a class="paper_link" href="https://faculty.washington.edu/wobbrock/pubs/gi-20.pdf" target="_blank"> Gedit: Keyboard gestures for mobile text editing </a></span>
                <span class="description"><strong>Mingrui Zhang</strong>, Jacob O. Wobbrock
                    <br>
                    Proceedings of Graphics Interface <strong>(GI)</strong>, 2020 <a href="https://youtu.be/0cvelA-6F6E" target="_blank">Video</a> <a href="https://www.youtube.com/watch?v=03IGy_I9Xkc" target="_blank">My presentation</a>
                    <br>
                    <font color="green" class="detail"> We proposed a set of on-keyboard text editing gestures for the mobile keyboard, similar to the desktop keyboard shotcuts, including ring/letter/swipe gestures to facilitate fast editing tasks such as cursor-moving/copy/paste/cut/undo. Gedit provides one- and two-handed operation modes, and is also compatible with the gesture typing input. </font>
            </span></li>
        </div></td></tr>

        <!-- start of proj -->
        <img class="smallthumbnail" src="/assets/img/research/TTC.png" alt="" >
        <tr><td class="imgtd">
            <img class="thumbnail" src="/assets/img/research/TTC.png" alt="" >
        </td><td>
        <div class="descol">
            <li><span class="title"><a class="paper_link" href="https://faculty.washington.edu/wobbrock/pubs/uist-19.02.pdf" target="_blank"> Type, Then Correct: Intelligent Text Correction Techniques for Mobile Text Entry Using Neural Networks </a></span>
                <span class="description"><strong>Mingrui Zhang</strong>, He Wen, Jacob O. Wobbrock
                    <br>
                    The ACM Symposium on User Interface Software and Technology <strong>(UIST)</strong>, 2019 <a href="https://www.youtube.com/watch?v=ZgrugZf8jrc" target="_blank">My Presentation</a> <a href="https://youtu.be/87NijB2flSk" target="_blank">Demo</a> <a href="TTC.html">Project page</a>
                    <br>
                    <font color="green" class="detail">Instead of normal touch+cursor based correction process, why cannot we rethink of the correction interaction? In this paper, we present three novel interactions that allow the user to type the correction first, then apply it to the error place. Furthermore, we applied deep learning technology to enable automatic error detection for the interaction. <a href="https://github.com/DrustZ/CorrectionRNN" target="_blank">Our correction RNN model</a> </font>
            </span></li>
        </div></td></tr>
        <!-- end of proj -->
    </table>


<h2 id="ubiquitous"> Ubiquitous Text Entry üôå</h2>
<div class="hr" style="padding-bottom:0"></div>
    <table class="row">
        <!-- start of proj -->
        <img class="smallthumbnail" src="/assets/img/research/TypeAnywhere.png" alt="" >
        <tr><td class="imgtd">
            <img class="thumbnail" src="/assets/img/research/TypeAnywhere.png" alt="" >
        </td>
        <td><div class="descol">
            <li><span class="title"><a class="paper_link" href="/assets/pdfs/paper_typeanywhere.pdf" target="_blank"> TypeAnywhere: A QWERTY-Based Text Entry Solution for Ubiquitous Computing </a></span>
                <span class="description"><strong>Mingrui Zhang</strong>, Shumin Zhai, Jacob O. Wobbrock
                    <br>
                    The ACM CHI Conference on Human Factors in Computing Systems <strong>(CHI)</strong>, 2022 <a href="https://youtu.be/WDIp7moK0wo" target="_blank">Demo</a> <a href="https://www.youtube.com/watch?v=SnCXNpBhM2Q" target="_blank">My presentation</a> <a href="https://github.com/DrustZ/TenFingerTyping" target="_blank">Project page</a>
                    <br>
                    <font color="green" class="detail">TypeAnywhere is a QWERTY-based text entry system for off-desktop computing environments. Using a wearable device that can detect finger taps, users can leverage their touch-typing skills from physical keyboards to perform text entry on any surface. The average performance was 59.7 WPM (84.8% of the physical keyboard speed).</font>
            </span></li>
        </div></td></tr>
        <!-- end of proj -->

        <!-- start of proj -->
        <img class="smallthumbnail" src="/assets/img/research/ATK.jpg" alt="" >
        <tr><td class="imgtd">
            <img class="thumbnail" src="/assets/img/research/ATK.jpg" alt="" >
        </td>
        <td><div class="descol">
            <li><span class="title"><a class="paper_link" href="http://pi.cs.tsinghua.edu.cn/lab/papers/p539-yi.pdf" target="_blank"> ATK: Enabling Ten-Finger Freehand Typing in Air Based on 3D Hand Tracking Data </a></span>
                <span class="description">Xin Yi, Chun Yu, <strong>Mingrui Zhang</strong>, Sida Gao
                    <br>
                    The ACM Symposium on User Interface Software and Technology <strong>(UIST)</strong>, 2015 <a href="https://www.youtube.com/watch?v=gYkSOzKY1LQ" target="_blank">Demo</a>
                    <br>
                    <font color="green" class="detail">A novel air-typing method, Leapmotion tracking fingers, improved Bayes prediction model with application developed. Users reached the speed of 29.2 WPM on average.</font>
            </span></li>
        </div></td></tr>
        <!-- end of proj -->
    </table>

<h2 id="eval"> Evalutating Text-based Communication Systems üßê</h2>
<div class="hr" style="padding-bottom:0"></div>
    <table class="row">

        <img class="smallthumbnail" src="/assets/img/research/emojitext.png" alt="" >
        <tr><td class="imgtd">
            <img class="thumbnail" src="/assets/img/research/emojitext.png" alt="" >
        </td>
        <td>
        <div class="descol">
            <li><span class="title"><a class="paper_link" href="/assets/pdfs/Emoji_for_iConf_2021.pdf" target="_blank"> A comparative study of lexical and semantic emoji suggestion systems </a></span>
                <span class="description"><strong>Mingrui Zhang</strong>, Alex Mariakakis, Jacob Burke, Jacob O. Wobbrock
                    <br>
                    <strong>iConference</strong>, 2021 
                    <a href="https://youtu.be/U55l5KPPB2A" target="_blank">My presentation</a>
                    <br>
                    <font color="green" class="detail"> We compared how the lexical based and semantic based emoji suggestion mechanisms affected the online chatting experience through an in-lab study and a field deployment. The results showed that the suggestion system of emojis did not influence the chatting experience, and users enjoy using both suggestion systems for different reasons. </font>
            </span></li>
        </div></td></tr>

        <img class="smallthumbnail" src="/assets/img/research/Tseq.png" alt="" >
        <tr><td class="imgtd">
            <img class="thumbnail" src="/assets/img/research/Tseq.png" alt="" >
        </td>
        <td><div class="descol">
            <li><span class="title"><a class="paper_link" href="https://faculty.washington.edu/wobbrock/pubs/uist-19.01.pdf" target="_blank">Beyond the Input Stream: Making Text Entry Evaluations More Flexible with Transcription Sequences </a></span>
                <span class="description"><strong>Mingrui Zhang</strong>, Jacob O. Wobbrock
                    <br>
                    The ACM Symposium on User Interface Software and Technology <strong>(UIST)</strong>, 2019 <a href="https://youtu.be/h1fJu-bP2wE" target="_blank">Demo</a>
                    <br>
                    <font color="green" class="detail"> In this work, we present a new underlying model that supersedes the input stream model for general-purpose method-independent character-level text entry evaluation. Specifically, we present an approach that replaces the input stream with transcription sequences, or ‚ÄúT-sequences‚Äù for short. In brief, T-sequences are snapshots of the entire transcribed string after each text-changing action is taken by the user. Every pair of successive snapshots are then analyzed to compute character-level text entry metrics. <a href="https://github.com/DrustZ/TextTestPP" target="_blank">TextTest++ platform</a></font> 
            </span></li>
        </div></td></tr>
    
        <img class="smallthumbnail" src="/assets/img/research/Throughput.png" alt="" >
        <tr><td class="imgtd">
            <img class="thumbnail" src="/assets/img/research/Throughput.png" alt="" >
        </td>
        <td><div class="descol">
            <li><span class="title"><a class="paper_link" href="http://faculty.washington.edu/wobbrock/pubs/chi-19.03.pdf" target="_blank">Text Entry Throughput: Towards Unifying Speed and Accuracy in a Single Performance Metric </a></span>
                <span class="description"><strong>Mingrui Zhang</strong>, Shumin Zhai, Jacob O. Wobbrock
                    <br>
                    The ACM CHI Conference on Human Factors in Computing Systems <strong>(CHI)</strong>, 2019 <a href="https://youtu.be/zFxdOLkI1dw" target="_blank">My presentation</a> <a href="/posts/2019/04/28/Throughput1/" target="_blank">Related blog post</a>
                    <br>
                    <font color="green" class="detail">We define the text entry Throughput as a performance metric combining the speed and accuracy. Throughput is derived from the transmission ratio in the information theory. Unlike other metrics, throughput is less affected by speed-accuracy tradeoffs, thus it enables cross-device, cross-publication comparison. <a href="https://github.com/DrustZ/Throughput" target="_blank">Throughput calculation library</a></font>
            </span></li>
        </div></td></tr>
    </table>

<h2 id="a11y"> Assistive Digital Communication Systems üë®‚Äçü¶Ø</h2>
<div class="hr" style="padding-bottom:0"></div>
    <table class="row">
        
        <img class="smallthumbnail" src="/assets/img/research/Ga11y.png" alt="" >
        <tr><td class="imgtd">
            <img class="thumbnail" src="/assets/img/research/Ga11y.png" alt="" >
        </td>
        <td><div class="descol">
            <li><span class="title"><a class="paper_link" href="/assets/pdfs/paper_ga11y.pdf" target="_blank">Ga11y: an Automated GIF Annotation System for Visually Impaired Users</a></span>
                <span class="description"><strong>Mingrui Zhang</strong>, Mingyuan Zhong, Jacob O. Wobbrock
                    <br>
                    The ACM CHI Conference on Human Factors in Computing Systems <strong>(CHI)</strong>, 2022 
                    <a href="https://www.youtube.com/watch?v=h0UCfPGNEqE" target="_blank">Demo</a> 
                    <a href="https://www.youtube.com/watch?v=QO4tGefqKYA" target="_blank">My presentation</a>
                    <a href="https://github.com/DrustZ/Ga11y" target="_blank"> Project page </a>
                    <br>
                    <font color="green" class="detail">Ga11y (pronounced as Gally) is an automatic GIF annotation system for blind or low vision users, combining the power of machine and human intelligence. It has an Android client, a remote server and an annotation web interface. For a GIF annotation request, Ga11y first responds with computer vision generated results  and uses crowdworkers to improve the annotation quality. </font>
            </span></li>
        </div></td></tr>

        <img class="smallthumbnail" src="/assets/img/research/voicemoji.png" alt="" >
        <tr><td class="imgtd">
            <img class="thumbnail" src="/assets/img/research/voicemoji.png" alt="" >
        </td>
        <td><div class="descol">
            <li><span class="title"><a class="paper_link" href="/assets/pdfs/voicemoji.pdf" target="_blank">Voicemoji: Emoji entry using voice for visually impaired people </a></span>
                <span class="description"><strong>Mingrui Zhang</strong>, Ruolin Wang, Xuhai Xu, Qisheng Li, Ather Sharif, Jacob O. Wobbrock
                    <br>
                    The ACM CHI Conference on Human Factors in Computing Systems <strong>(CHI)</strong>, 2021 
                    <a href="https://youtu.be/5cZa_eo5urM" target="_blank">My presentation</a> 
                    <a href="https://zhuanlan.zhihu.com/p/350906256" target="_blank"> Related blog post (in Chinese) </a>
                    <a href="https://github.com/DrustZ/VoiceEmoji" target="_blank"> Project page </a>
                    <br>
                    <font color="green" class="detail"> A speech-based emoji input system, designed for blind or low vision users. Use natural language style emoji query and context sensitive emoji suggestions based on the spoken content. Voicemoji speeds up the emoji entry process by 91% than the iOS keyboard.</font>
            </span></li>
        </div></td></tr>

        <img class="smallthumbnail" src="/assets/img/research/revamp.png" alt="" >
        <tr><td class="imgtd">
            <img class="thumbnail" src="/assets/img/research/revamp.png" alt="" >
        </td>
        <td><div class="descol">
            <li><span class="title"><a class="paper_link" href="https://arxiv.org/pdf/2102.00576.pdf" target="_blank">Revamp: Enhancing Accessible Information Seeking Experience of Online Shopping for Blind or Low Vision Users </a></span>
                <span class="description">Ruolin Wang, Zixuan Chen, <strong>Mingrui Zhang</strong>, Zhaoheng Li, Zhixu Liu, Zihang Dang, Chun Yu, Xiang "Anthony" Chen
                    <br>
                    The ACM CHI Conference on Human Factors in Computing Systems <strong>(CHI)</strong>, 2021
                    <br>
                    <font color="green" class="detail">Revamp is a online shopping system aimed to provide simplified experience for Blind and Low Vision (BLV) users. It extracts the user review from the product page using linguistic rules, and generate QA interfaces based on the review data, which provides the visual appearance information of the product. </font>
            </span></li>
        </div></td></tr>
    </table>

<h2 id="assistant"> Talking with Smart Assistants ü§ñ</h2>
<div class="hr" style="padding-bottom:0"></div>
    <table class="row">
        <img class="smallthumbnail" src="/assets/img/research/bungo.png" alt="" >
        <tr><td class="imgtd">
            <img class="thumbnail" src="/assets/img/research/bungo.png" alt="" >
        </td>
        <td><div class="descol">
            <li><span class="title"><a class="paper_link" href="http://faculty.washington.edu/alexisr/CATransfer.pdf" target="_blank">Can Conversational Agents Change the Way Children Talk to People?</a></span>
                <span class="description">Alexis Hiniker, Amelia Wang, Jonathan Tran, <strong>Mingrui Zhang</strong>, Jenny Redesky, Kiley Sobel, Sungsoo Ray Hong
                    <br>
                    Proceedings of the 20th annual ACM conference on interaction design and children <strong>(IDC)</strong>, 2021 <a href="https://www.washington.edu/news/2021/09/13/alexa-siri-make-kids-bossier-research-suggests-you-might-not-need-to-worry/" target="_blank"> Related UW News </a>
                    <br>
                    <font color="green" class="detail"> Does hanging out with Alexa or Siri affect the language routine children use to communicate with their fellow humans? In this work, we built two conversational agents, and let them teach the kids a word "bungo" to make the the agenets speak quickly. Although the kids used the word with the agents, they were aware of the social context when facing similar situation with other people. </font>
            </span></li>
        </div></td></tr>


        <img class="smallthumbnail" src="/assets/img/research/assumptions.png" alt="" >
        <tr><td class="imgtd">
            <img class="thumbnail" src="/assets/img/research/assumptions.png" alt="" >
        </td><td>
        <div class="descol">
            <li><span class="title"><a class="paper_link" href="http://bigyipper.com/wp-content/uploads/2020/02/imwut19a-sub8918-cam-i26.pdf" target="_blank">Assumptions Checked: How Families Learn About and Use the Echo Dot </a></span>
                <span class="description">Erin Beneteau, Yini Guan, Olivia K. Richards, <strong>Mingrui Zhang</strong>, Julie A. Kientz, Jason Yip, Alexis Hiniker
                    <br>
                    Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies <strong>(IMWUT)</strong>, 2020
                    <br>
                    <font color="green" class="detail"> Through a one-month deployment study, we investigated how families learnt new functionalities of smart speakers, include 1) which features families are aware of and engage with, and 2) how families explore, discover, and learn to use the Echo Dot. Drawing from diffusion of innovation theory, we describe how a home-based voice interface might be positioned as a near-peer to the user and help them discover new functionalities.</font>
            </span></li>
        </div></td></tr>

        <img class="smallthumbnail" src="/assets/img/research/alexa.jpg" alt="" >
        <tr><td class="imgtd">
            <img class="thumbnail" src="/assets/img/research/alexa.jpg" alt="" >
        </td>
        <td><div class="descol">
            <li><span class="title"><a class="paper_link" href="http://faculty.washington.edu/alexisr/commBreakdowns.pdf" target="_blank">Communication Breakdowns Between Families and Alexa </a></span>
                <span class="description">Erin Beneteau, Olivia K. Richards, <strong>Mingrui Zhang</strong>, Julie A. Kientz, Jason Yip, Alexis Hiniker
                    <br>
                    The ACM CHI Conference on Human Factors in Computing Systems <strong>(CHI)</strong>, 2019 <a href="https://www.youtube.com/watch?v=OWheOorTI88" target="_blank">Erin's Presentation</a> <a href="https://joanganzcooneycenter.org/2019/10/21/alexa-lets-work-on-your-communication-skills/" target="_blank"> Related Blog Post </a>
                    <br>
                    <font color="green" class="detail"> We investigated different types of communication breakdowns and the repairing strategies between the conversation of family members and Alexa. Our findings indicates that improving technology‚Äôs ability to identify the communication partners and to provide specific clarification responses will ultimately improve the conversational interaction experience.</font>
            </span></li>
        </div></td></tr>
    </table>

<h2 id="wellbeing"> Digital Information Wellbeing üßòüèª</h2>
<div class="hr" style="padding-bottom:0"></div>
    <table class="row">
        <img class="smallthumbnail" src="/assets/img/research/covid_newsreading.png" alt="" >
        <tr><td class="imgtd">
            <img class="thumbnail" src="/assets/img/research/covid_newsreading.png" alt="" >
        </td><td><div class="descol">
            <li><span class="title"><a class="paper_link" href="https://arxiv.org/abs/2202.05324" target="_blank">Understanding the Digital News Consumption Experience During the COVID Pandemic</a></span>
                <span class="description"><strong>Mingrui Zhang</strong>, Ashley Boone, Sara M Behbakht, Alexis Hiniker
                    <br>
                    Preprint, arXiv 2022
                    <br>
                    <font color="green" class="detail">We conduced a two-week diary study to investigate how people sought information through digital news platforms, and find that the consumption experience followed two stages:  the seeking stage, where participants increased their general consumption, motivated by three common informational needs -- specifically, to find, understand and verify relevant news pieces. Participants then moved to the sustaining stage, and coping with the news emotionally became as important as their informational needs. We also proposed interface designs for improving the consumption experience.</span></li>
        </div></td></tr>

        <img class="smallthumbnail" src="/assets/img/research/Chirp.png" alt="" >
        <tr><td class="imgtd">
            <img class="thumbnail" src="/assets/img/research/Chirp.png" alt="" >
        </td><td><div class="descol">
            <li><span class="title"><a class="paper_link" href="/assets/pdfs/paper_chirp.pdf" target="_blank">Monitoring Screen Time or Redesigning It? Two Approaches to Supporting Intentional Social Media Use</a></span>
                <span class="description"><strong>Mingrui Zhang</strong>, Kai Lukoff, Raveena Rao, Amanda Baughan, Alexis Hiniker
                    <br>
                    The ACM CHI Conference on Human Factors in Computing Systems <strong>(CHI)</strong>, 2022 
                    <a href="https://www.youtube.com/watch?v=g6PUeE0RN2Y" target="_blank"> My presentation</a>
                    <a href="https://github.com/uelab/Chirp" target="_blank">Project page</a>
                    <br>
                    <font color="green" class="detail">We design and deploy Chirp, a mobile Twitter client, to independently examine how users experience external (that monitor and limit the app usage) and internal (that change the interface/features of the app itself, such as list, filters and reading clues) supports. Our findings suggest that design patterns promoting agency may serve users better than screen time tools.</span></li>
        </div></td></tr>

        <img class="smallthumbnail" src="/assets/img/research/Dissociation.png" alt="" >
        <tr><td class="imgtd">
            <img class="thumbnail" src="/assets/img/research/Dissociation.png" alt="" >
        </td><td><div class="descol">
            <li><span class="title"><a class="paper_link" href="/assets/pdfs/paper_dissociation.pdf" target="_blank">‚ÄúI Don't Even Remember What I Read‚Äù: How Design Influences Dissociation on Social Media</a></span>
                <span class="description">Amanda Baughan, <strong>Mingrui Zhang</strong>, Raveena Rao, Kai Lukoff, Anastasia Schaadhardt, Lisa D. Butler, Alexis Hiniker
                    <br>
                    The ACM CHI Conference on Human Factors in Computing Systems <strong>(CHI)</strong>, 2022 
                    <br>
                    <font color="green" class="detail">Many people have experienced mindlessly scrolling on social media. We investigated these experiences through the lens of normative dissociation. We found that designed interventions--including custom lists, reading history labels, time limit dialogs, and usage statistics--reduced normative dissociation.</span></li>
        </div></td></tr>
    </table>

<h2 id="misc"> MISC üí°</h2>
<div class="hr" style="padding-bottom:0"></div>
    <table class="row">

        <img class="smallthumbnail" src="/assets/img/research/AttentionExplorer.jpg" alt="" >
        <tr><td class="imgtd">
            <img class="thumbnail" src="/assets/img/research/AttentionExplorer.jpg" alt="" >
        </td>
        <td><div class="descol">
            <li><span class="title"><a class="paper_link" href="/assets/pdfs/2020-pvis-attention.pdf" target="_blank">InteractiveAttention Model Explorer for NLP Tasks with Unbalanced Data Sizes </a></span>
                <span class="description">Zhihang Dong, Tongshuang Wu, Sicheng Song, <strong>Mingrui Zhang</strong>
                    <br>
                    IEEE Pacific Visualization Symposium <strong>(PacificVis)</strong>, Notes, 2020
                    <br>
                    <font color="green" class="detail"> We provide an intuitive visualization tool for natural language processing tasks where attention is mapped between documents with imbalanced sizes. We extend the flow map visualization to enhance the readability of the attention-augmented documents. <a href="https://cse512-18s.github.io/attention-model-explorer/" target="_blank">Our project page</a></font>
            </span></li>
        </div></td></tr>

        <img class="smallthumbnail" src="/assets/img/research/AAS.png" alt="" >
        <tr><td class="imgtd">
            <img class="thumbnail" src="/assets/img/research/AAS.png" alt="" >
        </td>
        <td><div class="descol">
            <li><span class="title"><a class="paper_link" href="http://faculty.washington.edu/alexisr/anchoredAudioSampling.pdf" target="_blank">Anchored Audio Sampling: A Seamless Method for Exploring Children‚Äôs Thoughts During Deployment Studies </a></span>
                <span class="description">Alexis Hiniker, Jon E. Froehlich, <strong>Mingrui Zhang</strong>, Erin Beneteau
                    <br>
                    The ACM CHI Conference on Human Factors in Computing Systems <strong>(CHI)</strong>, 2019 <img class="inline" src="/assets/img/research/bestpaper.jpg"><i>Best Paper Award</i>
                    <br>
                    <font color="green" class="detail">We present Anchored Audio Smapling (AAS) method for collecting remote data of qualitative audio samples during field development with young children. The anchor event triggers the recording, and a sliding window surrounding this anchor captures both antecedent and ensuing recording. <a href="https://github.com/uelab/KidsRecorder" target="_blank">Our AAS Library for Android</a></font>
            </span></li>
        </div></td></tr>
    </table>